---
phase: 05-postgres-agent-database-monitoring
plan: 02
type: execute
wave: 1
depends_on: []
files_modified:
  - postgres-agent/src/collectors/__init__.py
  - postgres-agent/src/collectors/pg_activity.py
  - postgres-agent/src/collectors/pg_statements.py
  - postgres-agent/src/collectors/pg_locks.py
  - postgres-agent/src/collectors/system_metrics.py
autonomous: true

must_haves:
  truths:
    - "Agent queries pg_stat_activity for active queries every collection cycle"
    - "Agent queries pg_stat_statements for query performance statistics"
    - "Agent detects blocking queries and lock contention"
    - "Agent collects CPU, memory, and disk I/O metrics"
    - "Agent extracts correlation IDs from application_name field"
  artifacts:
    - path: "postgres-agent/src/collectors/pg_activity.py"
      provides: "pg_stat_activity collector with correlation ID extraction"
      exports: ["collect_pg_activity"]
    - path: "postgres-agent/src/collectors/pg_statements.py"
      provides: "pg_stat_statements collector with graceful degradation"
      exports: ["collect_pg_statements", "check_pg_stat_statements"]
    - path: "postgres-agent/src/collectors/pg_locks.py"
      provides: "Lock detection with blocking query identification"
      exports: ["detect_blocking_queries"]
    - path: "postgres-agent/src/collectors/system_metrics.py"
      provides: "System metrics via psutil"
      exports: ["collect_system_metrics"]
  key_links:
    - from: "postgres-agent/src/collectors/pg_activity.py"
      to: "pg_stat_activity"
      via: "SQL query with correlation ID extraction"
      pattern: "application_name.*bitville"
    - from: "postgres-agent/src/collectors/pg_locks.py"
      to: "pg_locks and pg_stat_activity"
      via: "Lock monitoring query from PostgreSQL wiki"
      pattern: "blocked_locks\\.pid"
---

<objective>
Create data collectors for PostgreSQL statistics and system metrics.

Purpose: Implement the core data collection modules that query pg_stat_activity (PG-01), pg_stat_statements (PG-02), detect locks (PG-06), collect system metrics (PG-04), and extract correlation IDs (PG-05).

Output: Four collector modules that can be called from the main daemon loop to gather monitoring data.
</objective>

<execution_context>
@/Users/andrewvonhoesslin/.claude/get-shit-done/workflows/execute-plan.md
@/Users/andrewvonhoesslin/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/05-postgres-agent-database-monitoring/05-RESEARCH.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create pg_stat_activity collector with correlation ID extraction</name>
  <files>
    postgres-agent/src/collectors/__init__.py
    postgres-agent/src/collectors/pg_activity.py
  </files>
  <action>
Create the pg_stat_activity collector that extracts correlation IDs from application_name.

1. Create `postgres-agent/src/collectors/__init__.py`:
   ```python
   """Data collectors for PostgreSQL monitoring."""
   from .pg_activity import collect_pg_activity
   from .pg_statements import collect_pg_statements, check_pg_stat_statements
   from .pg_locks import detect_blocking_queries
   from .system_metrics import collect_system_metrics

   __all__ = [
       'collect_pg_activity',
       'collect_pg_statements',
       'check_pg_stat_statements',
       'detect_blocking_queries',
       'collect_system_metrics',
   ]
   ```

2. Create `postgres-agent/src/collectors/pg_activity.py`:
   ```python
   """
   pg_stat_activity collector.

   Queries pg_stat_activity for active queries and extracts correlation IDs
   from application_name for linking to PHP requests.

   PG-01: Query pg_stat_activity every minute for active queries and locks
   PG-05: Match correlation IDs from PHP via application_name parameter
   """
   import re
   from typing import Any
   from psycopg_pool import ConnectionPool
   import structlog

   logger = structlog.get_logger()

   # Pattern to extract correlation ID from application_name
   # PHP agent sets: "bitville-{correlation_id}"
   CORRELATION_PATTERN = re.compile(r'bitville-([a-f0-9-]{36})')


   def collect_pg_activity(pool: ConnectionPool) -> list[dict[str, Any]]:
       """
       Collect active queries from pg_stat_activity.

       Extracts correlation IDs from application_name field for PHP request linking.
       Only collects non-idle connections to minimize overhead.

       Args:
           pool: Database connection pool

       Returns:
           List of active session dicts with correlation_id if present
       """
       query = """
           SELECT
               pid,
               usename,
               application_name,
               client_addr,
               client_port,
               backend_start,
               xact_start,
               query_start,
               state_change,
               wait_event_type,
               wait_event,
               state,
               query,
               backend_type
           FROM pg_stat_activity
           WHERE state != 'idle'
             AND pid != pg_backend_pid()
           ORDER BY query_start DESC NULLS LAST
           LIMIT 100
       """

       try:
           with pool.connection() as conn:
               with conn.cursor() as cur:
                   cur.execute(query)
                   columns = [desc[0] for desc in cur.description]
                   rows = cur.fetchall()

           results = []
           for row in rows:
               record = dict(zip(columns, row))

               # Extract correlation ID from application_name (PG-05)
               app_name = record.get('application_name', '')
               match = CORRELATION_PATTERN.search(app_name) if app_name else None
               record['correlation_id'] = match.group(1) if match else None

               # Convert timestamps to ISO format strings
               for ts_field in ['backend_start', 'xact_start', 'query_start', 'state_change']:
                   if record.get(ts_field):
                       record[ts_field] = record[ts_field].isoformat()

               # Convert IP address to string
               if record.get('client_addr'):
                   record['client_addr'] = str(record['client_addr'])

               results.append(record)

           logger.debug(
               "pg_activity_collected",
               active_sessions=len(results),
               with_correlation=sum(1 for r in results if r.get('correlation_id'))
           )

           return results

       except Exception as e:
           logger.error("pg_activity_collection_failed", error=str(e))
           raise
   ```
  </action>
  <verify>
    - File exists: postgres-agent/src/collectors/__init__.py
    - File exists: postgres-agent/src/collectors/pg_activity.py
    - pg_activity.py contains CORRELATION_PATTERN regex for UUID extraction
    - pg_activity.py queries pg_stat_activity with state != 'idle' filter
    - pg_activity.py extracts correlation_id from application_name
  </verify>
  <done>pg_stat_activity collector with correlation ID extraction from application_name</done>
</task>

<task type="auto">
  <name>Task 2: Create pg_stat_statements collector with graceful degradation</name>
  <files>
    postgres-agent/src/collectors/pg_statements.py
  </files>
  <action>
Create pg_stat_statements collector that gracefully handles missing extension.

Create `postgres-agent/src/collectors/pg_statements.py`:
```python
"""
pg_stat_statements collector.

Queries pg_stat_statements for query performance statistics.
Gracefully degrades if extension is not installed.

PG-02: Query pg_stat_statements for query performance statistics
"""
from typing import Any, Optional
from psycopg_pool import ConnectionPool
import structlog

logger = structlog.get_logger()

# Cache extension availability check
_extension_available: Optional[bool] = None


def check_pg_stat_statements(pool: ConnectionPool) -> bool:
    """
    Check if pg_stat_statements extension is installed.

    Result is cached after first check.

    Args:
        pool: Database connection pool

    Returns:
        True if extension is available
    """
    global _extension_available

    if _extension_available is not None:
        return _extension_available

    try:
        with pool.connection() as conn:
            with conn.cursor() as cur:
                cur.execute("""
                    SELECT COUNT(*) FROM pg_extension
                    WHERE extname = 'pg_stat_statements'
                """)
                result = cur.fetchone()
                _extension_available = result is not None and result[0] > 0

        if _extension_available:
            logger.info("pg_stat_statements_available")
        else:
            logger.warning(
                "pg_stat_statements_not_installed",
                hint="Install with: CREATE EXTENSION pg_stat_statements;"
            )

        return _extension_available

    except Exception as e:
        logger.error("pg_stat_statements_check_failed", error=str(e))
        _extension_available = False
        return False


def collect_pg_statements(pool: ConnectionPool, limit: int = 100) -> list[dict[str, Any]]:
    """
    Collect query performance statistics from pg_stat_statements.

    Returns top queries by total execution time.
    Returns empty list if extension is not installed (graceful degradation).

    Args:
        pool: Database connection pool
        limit: Maximum number of queries to return (default: 100)

    Returns:
        List of query statistics dicts
    """
    if not check_pg_stat_statements(pool):
        return []

    # Query works with pg_stat_statements 1.8+ (PostgreSQL 13+)
    # Includes: calls, total_exec_time, mean_exec_time, rows, etc.
    query = """
        SELECT
            queryid,
            query,
            calls,
            total_exec_time,
            mean_exec_time,
            min_exec_time,
            max_exec_time,
            stddev_exec_time,
            rows,
            shared_blks_hit,
            shared_blks_read,
            shared_blks_written,
            local_blks_hit,
            local_blks_read,
            local_blks_written,
            temp_blks_read,
            temp_blks_written,
            blk_read_time,
            blk_write_time
        FROM pg_stat_statements
        ORDER BY total_exec_time DESC
        LIMIT %s
    """

    try:
        with pool.connection() as conn:
            with conn.cursor() as cur:
                cur.execute(query, (limit,))
                columns = [desc[0] for desc in cur.description]
                rows = cur.fetchall()

        results = []
        for row in rows:
            record = dict(zip(columns, row))

            # Convert queryid to string (it's a bigint)
            if record.get('queryid'):
                record['queryid'] = str(record['queryid'])

            # Truncate long queries to prevent payload bloat
            if record.get('query') and len(record['query']) > 1000:
                record['query'] = record['query'][:1000] + '...[truncated]'

            results.append(record)

        logger.debug(
            "pg_statements_collected",
            statement_count=len(results),
            top_query_time_ms=results[0].get('total_exec_time') if results else 0
        )

        return results

    except Exception as e:
        logger.error("pg_statements_collection_failed", error=str(e))
        # Return empty list on error (graceful degradation)
        return []
```
  </action>
  <verify>
    - File exists: postgres-agent/src/collectors/pg_statements.py
    - Contains check_pg_stat_statements function for extension check
    - Contains collect_pg_statements function with LIMIT parameter
    - Returns empty list if extension not available (graceful degradation)
    - Query truncation at 1000 chars to prevent payload bloat
  </verify>
  <done>pg_stat_statements collector with graceful degradation when extension missing</done>
</task>

<task type="auto">
  <name>Task 3: Create lock detection and system metrics collectors</name>
  <files>
    postgres-agent/src/collectors/pg_locks.py
    postgres-agent/src/collectors/system_metrics.py
  </files>
  <action>
Create lock detection (PG-06) and system metrics (PG-04) collectors.

1. Create `postgres-agent/src/collectors/pg_locks.py`:
   ```python
   """
   Lock detection collector.

   Detects blocking queries and lock contention using pg_locks
   combined with pg_stat_activity.

   PG-06: Detect and report database locks and blocking queries
   """
   from typing import Any
   from psycopg_pool import ConnectionPool
   import structlog

   logger = structlog.get_logger()


   def detect_blocking_queries(pool: ConnectionPool) -> list[dict[str, Any]]:
       """
       Detect blocking queries and lock contention.

       Uses the PostgreSQL wiki lock monitoring query to find
       blocked processes and their blockers.

       Source: https://wiki.postgresql.org/wiki/Lock_Monitoring

       Args:
           pool: Database connection pool

       Returns:
           List of blocking situations with blocker and blocked info
       """
       # PostgreSQL wiki lock monitoring query
       query = """
           SELECT
               blocked_locks.pid AS blocked_pid,
               blocked_activity.usename AS blocked_user,
               blocked_activity.application_name AS blocked_application,
               blocked_activity.client_addr AS blocked_client_addr,
               blocked_activity.query AS blocked_query,
               blocked_activity.query_start AS blocked_query_start,
               blocking_locks.pid AS blocking_pid,
               blocking_activity.usename AS blocking_user,
               blocking_activity.application_name AS blocking_application,
               blocking_activity.client_addr AS blocking_client_addr,
               blocking_activity.query AS blocking_query,
               blocking_activity.query_start AS blocking_query_start,
               blocked_locks.locktype,
               blocked_locks.mode AS blocked_mode,
               blocking_locks.mode AS blocking_mode
           FROM pg_catalog.pg_locks blocked_locks
           JOIN pg_catalog.pg_stat_activity blocked_activity
               ON blocked_activity.pid = blocked_locks.pid
           JOIN pg_catalog.pg_locks blocking_locks
               ON blocking_locks.locktype = blocked_locks.locktype
               AND blocking_locks.database IS NOT DISTINCT FROM blocked_locks.database
               AND blocking_locks.relation IS NOT DISTINCT FROM blocked_locks.relation
               AND blocking_locks.page IS NOT DISTINCT FROM blocked_locks.page
               AND blocking_locks.tuple IS NOT DISTINCT FROM blocked_locks.tuple
               AND blocking_locks.virtualxid IS NOT DISTINCT FROM blocked_locks.virtualxid
               AND blocking_locks.transactionid IS NOT DISTINCT FROM blocked_locks.transactionid
               AND blocking_locks.classid IS NOT DISTINCT FROM blocked_locks.classid
               AND blocking_locks.objid IS NOT DISTINCT FROM blocked_locks.objid
               AND blocking_locks.objsubid IS NOT DISTINCT FROM blocked_locks.objsubid
               AND blocking_locks.pid != blocked_locks.pid
           JOIN pg_catalog.pg_stat_activity blocking_activity
               ON blocking_activity.pid = blocking_locks.pid
           WHERE NOT blocked_locks.granted
           ORDER BY blocked_activity.query_start
           LIMIT 50
       """

       try:
           with pool.connection() as conn:
               with conn.cursor() as cur:
                   cur.execute(query)
                   columns = [desc[0] for desc in cur.description]
                   rows = cur.fetchall()

           results = []
           for row in rows:
               record = dict(zip(columns, row))

               # Convert timestamps to ISO format
               for ts_field in ['blocked_query_start', 'blocking_query_start']:
                   if record.get(ts_field):
                       record[ts_field] = record[ts_field].isoformat()

               # Convert IP addresses to strings
               for addr_field in ['blocked_client_addr', 'blocking_client_addr']:
                   if record.get(addr_field):
                       record[addr_field] = str(record[addr_field])

               # Truncate long queries
               for query_field in ['blocked_query', 'blocking_query']:
                   if record.get(query_field) and len(record[query_field]) > 500:
                       record[query_field] = record[query_field][:500] + '...[truncated]'

               results.append(record)

           if results:
               logger.warning(
                   "blocking_queries_detected",
                   count=len(results),
                   blocked_pids=[r['blocked_pid'] for r in results]
               )
           else:
               logger.debug("no_blocking_queries_detected")

           return results

       except Exception as e:
           logger.error("lock_detection_failed", error=str(e))
           return []
   ```

2. Create `postgres-agent/src/collectors/system_metrics.py`:
   ```python
   """
   System metrics collector using psutil.

   Collects CPU, memory, and disk I/O metrics from the database server.

   PG-04: Collect system metrics (CPU, RAM, disk I/O) on DB server
   """
   from typing import Any
   import psutil
   import structlog

   logger = structlog.get_logger()


   def collect_system_metrics() -> dict[str, Any]:
       """
       Collect system metrics using psutil.

       Returns CPU, memory, disk I/O, and network I/O metrics.

       Args:
           None

       Returns:
           Dict with system metrics
       """
       try:
           # CPU - use interval=1 for accurate percentage
           cpu_percent = psutil.cpu_percent(interval=1)
           cpu_count = psutil.cpu_count()
           cpu_count_logical = psutil.cpu_count(logical=True)

           # Load average (Unix only)
           try:
               load_avg = psutil.getloadavg()
           except (AttributeError, OSError):
               load_avg = (0, 0, 0)

           # Memory
           mem = psutil.virtual_memory()

           # Swap
           swap = psutil.swap_memory()

           # Disk I/O
           try:
               disk_io = psutil.disk_io_counters()
               disk_metrics = {
                   'read_count': disk_io.read_count,
                   'write_count': disk_io.write_count,
                   'read_bytes': disk_io.read_bytes,
                   'write_bytes': disk_io.write_bytes,
                   'read_time_ms': disk_io.read_time,
                   'write_time_ms': disk_io.write_time,
               } if disk_io else {}
           except Exception:
               disk_metrics = {}

           # Network I/O
           try:
               net_io = psutil.net_io_counters()
               network_metrics = {
                   'bytes_sent': net_io.bytes_sent,
                   'bytes_recv': net_io.bytes_recv,
                   'packets_sent': net_io.packets_sent,
                   'packets_recv': net_io.packets_recv,
                   'errin': net_io.errin,
                   'errout': net_io.errout,
                   'dropin': net_io.dropin,
                   'dropout': net_io.dropout,
               }
           except Exception:
               network_metrics = {}

           # Disk usage for common PostgreSQL paths
           disk_usage = {}
           for path in ['/var/lib/postgresql', '/var/log/postgresql', '/']:
               try:
                   usage = psutil.disk_usage(path)
                   disk_usage[path] = {
                       'total': usage.total,
                       'used': usage.used,
                       'free': usage.free,
                       'percent': usage.percent,
                   }
               except (FileNotFoundError, PermissionError):
                   pass

           result = {
               'cpu': {
                   'percent': cpu_percent,
                   'count_physical': cpu_count,
                   'count_logical': cpu_count_logical,
                   'load_avg_1m': load_avg[0],
                   'load_avg_5m': load_avg[1],
                   'load_avg_15m': load_avg[2],
               },
               'memory': {
                   'total': mem.total,
                   'available': mem.available,
                   'used': mem.used,
                   'percent': mem.percent,
                   'buffers': getattr(mem, 'buffers', 0),
                   'cached': getattr(mem, 'cached', 0),
               },
               'swap': {
                   'total': swap.total,
                   'used': swap.used,
                   'free': swap.free,
                   'percent': swap.percent,
               },
               'disk_io': disk_metrics,
               'network_io': network_metrics,
               'disk_usage': disk_usage,
           }

           logger.debug(
               "system_metrics_collected",
               cpu_percent=cpu_percent,
               memory_percent=mem.percent,
               load_1m=load_avg[0]
           )

           return result

       except Exception as e:
           logger.error("system_metrics_collection_failed", error=str(e))
           return {
               'error': str(e),
               'cpu': {},
               'memory': {},
               'swap': {},
               'disk_io': {},
               'network_io': {},
               'disk_usage': {},
           }
   ```
  </action>
  <verify>
    - File exists: postgres-agent/src/collectors/pg_locks.py
    - File exists: postgres-agent/src/collectors/system_metrics.py
    - pg_locks.py uses PostgreSQL wiki lock monitoring query
    - pg_locks.py returns empty list on error (graceful degradation)
    - system_metrics.py collects CPU, memory, disk I/O, network I/O
    - system_metrics.py includes load average for Unix systems
    - system_metrics.py handles missing metrics gracefully
  </verify>
  <done>Lock detection and system metrics collectors with graceful error handling</done>
</task>

</tasks>

<verification>
1. All four collector modules exist in postgres-agent/src/collectors/
2. pg_activity extracts correlation IDs from application_name using bitville-{uuid} pattern
3. pg_statements gracefully returns empty list if extension not installed
4. pg_locks uses official PostgreSQL wiki query for lock detection
5. system_metrics collects all specified metrics (CPU, RAM, disk I/O)
6. All collectors handle errors gracefully without crashing
</verification>

<success_criteria>
- pg_stat_activity collector extracts correlation IDs from application_name (PG-05)
- pg_stat_statements collector works or gracefully degrades (PG-02)
- Lock detection identifies blocking queries (PG-06)
- System metrics collection includes CPU, memory, disk I/O (PG-04)
- All collectors return serializable data structures
</success_criteria>

<output>
After completion, create `.planning/phases/05-postgres-agent-database-monitoring/05-02-SUMMARY.md`
</output>
