---
phase: 05-postgres-agent-database-monitoring
plan: 04
type: execute
wave: 3
depends_on: [05-01, 05-02, 05-03]
files_modified:
  - postgres-agent/src/daemon.py
  - postgres-agent/src/__main__.py
  - postgres-agent/systemd/postgres-agent.service
autonomous: true

must_haves:
  truths:
    - "Agent runs as daemon service on DB server"
    - "Agent collects all data sources every 60 seconds"
    - "Agent handles SIGTERM gracefully"
    - "Agent includes project identifier with all sent data"
    - "Agent never causes database failures or performance degradation"
  artifacts:
    - path: "postgres-agent/src/daemon.py"
      provides: "Main daemon loop with collection, transmission, and signal handling"
      exports: ["PostgresMonitoringAgent"]
    - path: "postgres-agent/src/__main__.py"
      provides: "Entry point for python -m postgres_agent"
      contains: "main()"
    - path: "postgres-agent/systemd/postgres-agent.service"
      provides: "systemd unit file for daemon"
      contains: "ExecStart"
  key_links:
    - from: "postgres-agent/src/daemon.py"
      to: "collectors and transmission modules"
      via: "import and call in collection loop"
      pattern: "collect_pg_activity.*collect_pg_statements.*collect_system_metrics"
    - from: "postgres-agent/systemd/postgres-agent.service"
      to: "daemon.py"
      via: "ExecStart with python -m"
      pattern: "ExecStart.*python.*postgres_agent"
---

<objective>
Create main daemon loop and systemd service configuration.

Purpose: Integrate all collectors and transmission into a production-ready daemon that runs on the DB server (PG-COMM-04), collects data every minute (PG-01), and handles lifecycle events gracefully (PG-07).

Output: Complete daemon that can be installed and run as a systemd service on the database server.
</objective>

<execution_context>
@/Users/andrewvonhoesslin/.claude/get-shit-done/workflows/execute-plan.md
@/Users/andrewvonhoesslin/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/05-postgres-agent-database-monitoring/05-RESEARCH.md
@.planning/phases/05-postgres-agent-database-monitoring/05-01-SUMMARY.md
@.planning/phases/05-postgres-agent-database-monitoring/05-02-SUMMARY.md
@.planning/phases/05-postgres-agent-database-monitoring/05-03-SUMMARY.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create main daemon loop with collection orchestration</name>
  <files>
    postgres-agent/src/daemon.py
  </files>
  <action>
Create the main daemon class that orchestrates collection and transmission.

Create `postgres-agent/src/daemon.py`:
```python
"""
PostgreSQL Monitoring Agent Daemon.

Main daemon loop that:
1. Collects data from multiple sources every 60 seconds
2. Transmits to listener with circuit breaker protection
3. Buffers locally during outages
4. Handles graceful shutdown

PG-COMM-04: Run as daemon service on DB server (5.9.121.222)
PG-07: Never cause database failures or performance degradation
"""
import asyncio
import signal
import sys
import threading
import time
from typing import Any, Optional
import structlog

from .config import Config, load_config
from .database.pool import create_pool, close_pool, get_pool
from .collectors import (
    collect_pg_activity,
    collect_pg_statements,
    check_pg_stat_statements,
    detect_blocking_queries,
    collect_system_metrics,
)
from .collectors.log_parser import tail_postgres_log, LogCollector
from .transmission import send_to_listener, is_circuit_open, flush_buffer
from .transmission.buffer import init_buffer, get_buffer_stats, close_buffer

logger = structlog.get_logger()


class PostgresMonitoringAgent:
    """
    PostgreSQL Monitoring Agent.

    Collects database statistics and system metrics,
    sends to central listener, buffers during outages.
    """

    def __init__(self, config: Config):
        """
        Initialize agent.

        Args:
            config: Agent configuration
        """
        self.config = config
        self.running = False
        self._shutdown_event = threading.Event()
        self._log_thread: Optional[threading.Thread] = None
        self._log_collector = LogCollector(max_entries=500)

    def start(self):
        """
        Start the monitoring agent.

        Initializes connection pool, buffer, and starts collection loop.
        """
        logger.info(
            "agent_starting",
            project=self.config.project_id,
            db_host=self.config.db_host,
            listener_url=self.config.listener_url
        )

        # Register signal handlers
        signal.signal(signal.SIGTERM, self._handle_shutdown)
        signal.signal(signal.SIGINT, self._handle_shutdown)
        signal.signal(signal.SIGHUP, self._handle_reload)

        try:
            # Initialize connection pool with safety limits
            create_pool(self.config)

            # Initialize buffer
            init_buffer(self.config)

            # Check pg_stat_statements availability
            pool = get_pool()
            has_statements = check_pg_stat_statements(pool)
            logger.info(
                "pg_stat_statements_check",
                available=has_statements
            )

            self.running = True

            # Start log parsing thread (PG-03)
            self._start_log_parser_thread()

            # Start main collection loop
            self._collection_loop()

        except Exception as e:
            logger.error("agent_startup_failed", error=str(e))
            self._cleanup()
            raise

    def _start_log_parser_thread(self):
        """Start background thread for continuous log parsing."""
        def log_parser_worker():
            logger.info("log_parser_thread_started")
            try:
                for entry in tail_postgres_log(
                    self.config.postgres_log_path,
                    poll_interval=0.1
                ):
                    if self._shutdown_event.is_set():
                        break

                    # Add to collector buffer
                    should_flush = self._log_collector.add(entry)

                    # Flush if buffer full or has correlation ID (important)
                    if should_flush or entry.get('correlation_id'):
                        self._send_log_entries()

            except Exception as e:
                logger.error("log_parser_thread_error", error=str(e))

        self._log_thread = threading.Thread(
            target=log_parser_worker,
            daemon=True,
            name="log-parser"
        )
        self._log_thread.start()

    def _send_log_entries(self):
        """Send buffered log entries to listener."""
        entries = self._log_collector.flush()
        if not entries:
            return

        # Group entries by severity for sending
        log_data = {
            'entries': entries,
            'count': len(entries),
            'levels': {}
        }

        for entry in entries:
            level = entry.get('level', 'UNKNOWN')
            log_data['levels'][level] = log_data['levels'].get(level, 0) + 1

        send_to_listener(log_data, self.config, 'pg_log')

    def _collection_loop(self):
        """
        Main collection loop.

        Runs every collection_interval_s (default: 60 seconds).
        Collects from all sources and transmits to listener.
        """
        logger.info(
            "collection_loop_starting",
            interval_s=self.config.collection_interval_s
        )

        while self.running:
            cycle_start = time.time()

            try:
                self._collection_cycle()

            except Exception as e:
                logger.error("collection_cycle_failed", error=str(e))

            # Calculate sleep time to maintain interval
            elapsed = time.time() - cycle_start
            sleep_time = max(0, self.config.collection_interval_s - elapsed)

            logger.debug(
                "collection_cycle_complete",
                elapsed_s=round(elapsed, 2),
                sleep_s=round(sleep_time, 2)
            )

            # Wait for next cycle or shutdown
            if self._shutdown_event.wait(timeout=sleep_time):
                break

        logger.info("collection_loop_stopped")

    def _collection_cycle(self):
        """
        Single collection cycle.

        Collects from all sources, sends to listener.
        """
        pool = get_pool()
        results: dict[str, Any] = {}
        errors: list[str] = []

        # Collect pg_stat_activity (PG-01)
        try:
            results['pg_activity'] = collect_pg_activity(pool)
        except Exception as e:
            errors.append(f"pg_activity: {e}")
            results['pg_activity'] = []

        # Collect pg_stat_statements (PG-02)
        try:
            results['pg_statements'] = collect_pg_statements(pool)
        except Exception as e:
            errors.append(f"pg_statements: {e}")
            results['pg_statements'] = []

        # Detect locks (PG-06)
        try:
            results['locks'] = detect_blocking_queries(pool)
        except Exception as e:
            errors.append(f"locks: {e}")
            results['locks'] = []

        # Collect system metrics (PG-04)
        try:
            results['system_metrics'] = collect_system_metrics()
        except Exception as e:
            errors.append(f"system_metrics: {e}")
            results['system_metrics'] = {}

        # Log collection summary
        logger.info(
            "collection_complete",
            active_sessions=len(results['pg_activity']),
            statements=len(results['pg_statements']),
            locks=len(results['locks']),
            errors=errors if errors else None
        )

        # Send each data type to listener
        # pg_stat_activity
        if results['pg_activity']:
            send_to_listener(
                {
                    'sessions': results['pg_activity'],
                    'count': len(results['pg_activity']),
                    'with_correlation': sum(
                        1 for s in results['pg_activity']
                        if s.get('correlation_id')
                    )
                },
                self.config,
                'pg_stat_activity'
            )

        # pg_stat_statements
        if results['pg_statements']:
            send_to_listener(
                {
                    'statements': results['pg_statements'],
                    'count': len(results['pg_statements'])
                },
                self.config,
                'pg_stat_statements'
            )

        # Locks (always send, even if empty - important for alerting)
        send_to_listener(
            {
                'locks': results['locks'],
                'count': len(results['locks']),
                'has_blocking': len(results['locks']) > 0
            },
            self.config,
            'pg_locks'
        )

        # System metrics
        send_to_listener(
            results['system_metrics'],
            self.config,
            'system_metrics'
        )

        # Try to flush buffer if circuit is closed
        if not is_circuit_open():
            stats = get_buffer_stats()
            if stats.get('queue_size', 0) > 0:
                sent, remaining = flush_buffer(self.config, max_items=50)
                logger.info(
                    "buffer_flush_attempted",
                    sent=sent,
                    remaining=remaining
                )

    def _handle_shutdown(self, signum, frame):
        """Handle SIGTERM/SIGINT for graceful shutdown."""
        sig_name = signal.Signals(signum).name
        logger.info("shutdown_signal_received", signal=sig_name)
        self.running = False
        self._shutdown_event.set()

    def _handle_reload(self, signum, frame):
        """Handle SIGHUP for configuration reload."""
        logger.info("reload_signal_received")
        # For now, just log - could reload config in future

    def _cleanup(self):
        """Clean up resources on shutdown."""
        logger.info("cleanup_starting")

        # Flush remaining log entries
        if self._log_collector.count() > 0:
            self._send_log_entries()

        # Close connection pool
        try:
            close_pool()
        except Exception as e:
            logger.error("pool_close_failed", error=str(e))

        # Close buffer
        try:
            close_buffer()
        except Exception as e:
            logger.error("buffer_close_failed", error=str(e))

        logger.info("cleanup_complete")

    def stop(self):
        """Stop the agent gracefully."""
        logger.info("agent_stopping")
        self.running = False
        self._shutdown_event.set()
        self._cleanup()


def main(config_path: Optional[str] = None):
    """
    Main entry point.

    Args:
        config_path: Path to configuration file (optional)
    """
    # Configure structured logging
    structlog.configure(
        processors=[
            structlog.stdlib.add_log_level,
            structlog.stdlib.PositionalArgumentsFormatter(),
            structlog.processors.TimeStamper(fmt="iso"),
            structlog.processors.StackInfoRenderer(),
            structlog.processors.format_exc_info,
            structlog.processors.JSONRenderer()
        ],
        wrapper_class=structlog.stdlib.BoundLogger,
        context_class=dict,
        logger_factory=structlog.PrintLoggerFactory(),
        cache_logger_on_first_use=True
    )

    logger.info("postgres_agent_starting", version="1.0.0")

    # Load configuration
    config = load_config(config_path)

    # Validate required settings
    if not config.listener_api_key:
        logger.error("listener_api_key_required")
        sys.exit(1)

    # Start agent
    agent = PostgresMonitoringAgent(config)

    try:
        agent.start()
    except KeyboardInterrupt:
        logger.info("keyboard_interrupt")
    except Exception as e:
        logger.error("agent_failed", error=str(e))
        sys.exit(1)
    finally:
        agent.stop()

    logger.info("postgres_agent_stopped")
```
  </action>
  <verify>
    - File exists: postgres-agent/src/daemon.py
    - Contains PostgresMonitoringAgent class
    - Registers SIGTERM and SIGINT handlers
    - Starts log parser in background thread
    - Collection loop runs every collection_interval_s (60s default)
    - Collects from all sources: pg_activity, pg_statements, locks, system_metrics
    - Sends each data type separately to listener
    - Attempts buffer flush when circuit is closed
    - Cleanup closes pool and buffer
  </verify>
  <done>Main daemon with collection loop, signal handling, and graceful shutdown</done>
</task>

<task type="auto">
  <name>Task 2: Create entry point and systemd service</name>
  <files>
    postgres-agent/src/__main__.py
    postgres-agent/systemd/postgres-agent.service
  </files>
  <action>
Create entry point for `python -m postgres_agent` and systemd service configuration.

1. Create `postgres-agent/src/__main__.py`:
   ```python
   """
   Entry point for python -m postgres_agent

   Usage:
       python -m postgres_agent [config_path]

   Environment variables:
       BITVILLE_PG_CONFIG_PATH - Path to configuration file
       BITVILLE_PG_* - Configuration overrides (see config.py)
   """
   import os
   import sys

   from .daemon import main


   if __name__ == "__main__":
       # Get config path from argument or environment
       config_path = None

       if len(sys.argv) > 1:
           config_path = sys.argv[1]
       elif os.environ.get('BITVILLE_PG_CONFIG_PATH'):
           config_path = os.environ['BITVILLE_PG_CONFIG_PATH']

       main(config_path)
   ```

2. Create directory and file `postgres-agent/systemd/postgres-agent.service`:
   ```ini
   # Bitville PostgreSQL Monitoring Agent
   # systemd service unit file
   #
   # Installation:
   #   sudo cp postgres-agent.service /etc/systemd/system/
   #   sudo systemctl daemon-reload
   #   sudo systemctl enable postgres-agent
   #   sudo systemctl start postgres-agent
   #
   # Target server: 5.9.121.222 (DB server)

   [Unit]
   Description=Bitville PostgreSQL Monitoring Agent
   Documentation=https://github.com/your-org/bitville-monitoring
   After=network.target postgresql.service
   Wants=postgresql.service

   [Service]
   Type=simple
   User=bitville-agent
   Group=bitville-agent

   # Working directory and entry point
   WorkingDirectory=/opt/bitville-postgres-agent
   ExecStart=/usr/bin/python3 -m postgres_agent /etc/bitville/postgres-agent.ini

   # Environment configuration
   Environment=PYTHONUNBUFFERED=1
   EnvironmentFile=-/etc/bitville/postgres-agent.env

   # Restart policy
   Restart=on-failure
   RestartSec=10
   StartLimitBurst=5
   StartLimitIntervalSec=60

   # Security hardening (PG-07: never cause failures)
   NoNewPrivileges=true
   PrivateTmp=true
   ProtectSystem=strict
   ProtectHome=true

   # Allow write to buffer and log directories
   ReadWritePaths=/var/lib/bitville-postgres-agent
   ReadWritePaths=/var/log/bitville-postgres-agent

   # Allow read of PostgreSQL logs
   ReadOnlyPaths=/var/log/postgresql

   # Resource limits to prevent runaway
   MemoryLimit=256M
   MemoryHigh=200M
   TasksMax=10
   CPUQuota=25%

   # Logging to journal
   StandardOutput=journal
   StandardError=journal
   SyslogIdentifier=postgres-agent

   # Graceful shutdown timeout
   TimeoutStopSec=30

   [Install]
   WantedBy=multi-user.target
   ```
  </action>
  <verify>
    - File exists: postgres-agent/src/__main__.py
    - File exists: postgres-agent/systemd/postgres-agent.service
    - __main__.py accepts config path from argv or env
    - Service runs as bitville-agent user
    - Service has memory limit (256M) and CPU quota (25%)
    - Service has security hardening (NoNewPrivileges, ProtectSystem)
    - Service reads PostgreSQL logs (ReadOnlyPaths=/var/log/postgresql)
    - Service writes to buffer directory (ReadWritePaths)
  </verify>
  <done>Entry point and systemd service with security hardening and resource limits</done>
</task>

<task type="auto">
  <name>Task 3: Create installation and setup documentation</name>
  <files>
    postgres-agent/README.md
  </files>
  <action>
Create installation documentation for the Postgres agent.

Create `postgres-agent/README.md`:
```markdown
# Bitville PostgreSQL Monitoring Agent

A lightweight monitoring agent that collects database statistics, system metrics, and log data from PostgreSQL servers and sends them to the central Bitville listener.

## Features

- **pg_stat_activity polling** - Active queries, locks, connection state
- **pg_stat_statements** - Query performance statistics (if extension installed)
- **Lock detection** - Blocking queries and lock contention
- **System metrics** - CPU, memory, disk I/O via psutil
- **Log parsing** - Continuous PostgreSQL log file tailing
- **Correlation** - Links PHP requests via application_name
- **Resilience** - Circuit breaker + local buffering for outages

## Requirements

- Python 3.11+
- PostgreSQL 12+ (for full feature support)
- Access to pg_stat_activity (pg_read_all_stats role)
- Network access to listener server (88.198.22.206:8443)

## Quick Start

### 1. Install Dependencies

```bash
cd /opt/bitville-postgres-agent
pip install -r requirements.txt
```

### 2. Create Configuration

```bash
sudo mkdir -p /etc/bitville
sudo cp config/agent.ini.example /etc/bitville/postgres-agent.ini
sudo nano /etc/bitville/postgres-agent.ini
```

Edit configuration:
- Set `host` to your PostgreSQL host (or localhost)
- Set `user` and `password` for monitoring user
- Set `api_key` to your Bitville API key
- Set `project_id` to identify this server

### 3. Create Buffer Directory

```bash
sudo mkdir -p /var/lib/bitville-postgres-agent
sudo chown bitville-agent:bitville-agent /var/lib/bitville-postgres-agent
```

### 4. Create Monitoring User (PostgreSQL)

```sql
-- Run as superuser
CREATE USER bitville_monitor WITH PASSWORD 'secure_password';
GRANT pg_read_all_stats TO bitville_monitor;

-- For pg_stat_statements (optional)
CREATE EXTENSION IF NOT EXISTS pg_stat_statements;
```

### 5. Install systemd Service

```bash
sudo cp systemd/postgres-agent.service /etc/systemd/system/
sudo systemctl daemon-reload
sudo systemctl enable postgres-agent
sudo systemctl start postgres-agent
```

### 6. Verify

```bash
# Check service status
sudo systemctl status postgres-agent

# View logs
sudo journalctl -u postgres-agent -f

# Check listener is receiving data
curl https://88.198.22.206:8443/ready
```

## Configuration

### INI File

`/etc/bitville/postgres-agent.ini`:

```ini
[database]
host = localhost
port = 5432
name = postgres
user = bitville_monitor
password = your_password
statement_timeout_ms = 5000

[collection]
interval_s = 60
log_path = /var/log/postgresql/postgresql-main.log

[listener]
url = https://88.198.22.206:8443/ingest/postgres
api_key = your_api_key_here
project_id = myproject

[buffer]
path = /var/lib/bitville-postgres-agent/buffer
max_size_mb = 100
```

### Environment Variables

All settings can be overridden via environment:

```bash
BITVILLE_PG_DB_HOST=localhost
BITVILLE_PG_DB_PASSWORD=secret
BITVILLE_PG_LISTENER_API_KEY=your_key
BITVILLE_PG_PROJECT_ID=myproject
```

## Safety Guarantees

The agent is designed to **never** impact database performance:

1. **Statement timeout** - All queries timeout after 5 seconds
2. **Connection pool limit** - Maximum 5 connections
3. **Resource limits** - systemd enforces 256MB memory, 25% CPU
4. **Graceful degradation** - Missing extensions don't cause failures
5. **Circuit breaker** - Stops transmission attempts during outages

## Troubleshooting

### Agent won't connect to database

1. Check PostgreSQL is running: `pg_isready`
2. Verify credentials: `psql -U bitville_monitor -h localhost`
3. Check pg_hba.conf allows connection

### No data in listener

1. Check API key is correct
2. Verify network connectivity: `curl https://88.198.22.206:8443/health`
3. Check buffer for queued data: `ls -la /var/lib/bitville-postgres-agent/buffer/`

### pg_stat_statements not available

This is optional. Agent will log a warning and continue without query statistics.

To enable:
```sql
-- postgresql.conf
shared_preload_libraries = 'pg_stat_statements'

-- Then restart PostgreSQL and run:
CREATE EXTENSION pg_stat_statements;
```

### High memory usage

Check buffer size - if listener is unavailable, buffer may grow.

```bash
ls -lh /var/lib/bitville-postgres-agent/buffer/
```

Buffer auto-evicts oldest items when exceeding 100MB.

## Development

### Running locally

```bash
# With config file
python -m postgres_agent config/agent.ini.example

# With environment variables
export BITVILLE_PG_DB_HOST=localhost
export BITVILLE_PG_LISTENER_API_KEY=test
python -m postgres_agent
```

### Testing

```bash
# Run tests
pytest tests/

# Run with coverage
pytest --cov=src tests/
```

## License

MIT
```
  </action>
  <verify>
    - File exists: postgres-agent/README.md
    - Contains installation instructions
    - Contains configuration documentation
    - Contains PostgreSQL user creation SQL
    - Contains troubleshooting section
    - Documents safety guarantees
  </verify>
  <done>Installation documentation with setup instructions and troubleshooting</done>
</task>

</tasks>

<verification>
1. Daemon starts and runs collection loop every 60 seconds
2. All collectors called in each cycle
3. Data sent to listener with project identifier
4. Signal handlers work (SIGTERM, SIGINT)
5. systemd service has resource limits and security hardening
6. README contains complete installation instructions
</verification>

<success_criteria>
- Agent runs as daemon with 60-second collection interval (PG-01)
- Agent sends data with project identifier (PG-COMM-03)
- Agent runs as systemd service (PG-COMM-04)
- Agent never exceeds resource limits (PG-07)
- Complete installation documentation exists
</success_criteria>

<output>
After completion, create `.planning/phases/05-postgres-agent-database-monitoring/05-04-SUMMARY.md`
</output>
