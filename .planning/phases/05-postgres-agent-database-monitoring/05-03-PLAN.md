---
phase: 05-postgres-agent-database-monitoring
plan: 03
type: execute
wave: 2
depends_on: [05-01]
files_modified:
  - postgres-agent/src/collectors/log_parser.py
  - postgres-agent/src/transmission/__init__.py
  - postgres-agent/src/transmission/http_client.py
  - postgres-agent/src/transmission/circuit_breaker.py
  - postgres-agent/src/transmission/buffer.py
autonomous: true

must_haves:
  truths:
    - "Agent parses Postgres log files continuously"
    - "Agent sends data to listener via HTTP POST with Bearer token auth"
    - "Agent buffers locally when listener is unavailable"
    - "Circuit breaker opens after 5 consecutive failures"
    - "Buffer evicts oldest items when max size exceeded"
  artifacts:
    - path: "postgres-agent/src/collectors/log_parser.py"
      provides: "Continuous log file parser with rotation handling"
      exports: ["tail_postgres_log", "parse_log_line"]
    - path: "postgres-agent/src/transmission/http_client.py"
      provides: "HTTP client for listener with auth and timeout"
      exports: ["send_to_listener"]
    - path: "postgres-agent/src/transmission/circuit_breaker.py"
      provides: "Circuit breaker with 5-failure threshold"
      exports: ["get_circuit_breaker", "is_circuit_open"]
    - path: "postgres-agent/src/transmission/buffer.py"
      provides: "Persistent buffer with size limit and eviction"
      exports: ["get_buffer", "buffer_data", "flush_buffer"]
  key_links:
    - from: "postgres-agent/src/transmission/http_client.py"
      to: "listener /ingest/postgres endpoint"
      via: "requests POST with Bearer token"
      pattern: "Authorization.*Bearer"
    - from: "postgres-agent/src/transmission/buffer.py"
      to: "persist-queue FIFOSQLiteQueue"
      via: "SQLite-backed persistent queue"
      pattern: "FIFOSQLiteQueue"
---

<objective>
Create Postgres log parser and transmission layer with circuit breaker and buffering.

Purpose: Implement log file parsing (PG-03), HTTP transmission to listener (PG-COMM-01), local buffering during outages (PG-COMM-02), and circuit breaker pattern for resilience.

Output: Log parser that handles rotation, HTTP client with auth, circuit breaker, and persistent buffer with size limits.
</objective>

<execution_context>
@/Users/andrewvonhoesslin/.claude/get-shit-done/workflows/execute-plan.md
@/Users/andrewvonhoesslin/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/05-postgres-agent-database-monitoring/05-RESEARCH.md
@.planning/phases/05-postgres-agent-database-monitoring/05-01-SUMMARY.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create Postgres log parser with rotation handling</name>
  <files>
    postgres-agent/src/collectors/log_parser.py
  </files>
  <action>
Create log parser that continuously reads Postgres log files and handles log rotation.

Create `postgres-agent/src/collectors/log_parser.py`:
```python
"""
PostgreSQL log file parser.

Continuously reads and parses Postgres log files with log rotation handling.
Extracts query logs, errors, and slow queries.

PG-03: Parse Postgres log files continuously for query logs
"""
import os
import re
import time
from typing import Any, Generator, Optional
from datetime import datetime
import structlog

logger = structlog.get_logger()

# PostgreSQL log line patterns (common log_line_prefix formats)
# Format: timestamp [pid] level: message
LOG_LINE_PATTERN = re.compile(
    r'^(?P<timestamp>\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2}(?:\.\d+)?)'
    r'(?:\s+\w+)?'  # timezone
    r'\s+\[(?P<pid>\d+)\]'
    r'(?:\s+\[(?P<user>\w+)\])?'
    r'(?:\s+\[(?P<db>\w+)\])?'
    r'\s+(?P<level>\w+):\s+'
    r'(?P<message>.*)'
)

# Alternative simpler format
LOG_LINE_SIMPLE = re.compile(
    r'^(?P<timestamp>\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2})'
    r'.*?\[(?P<pid>\d+)\]'
    r'.*?(?P<level>LOG|ERROR|WARNING|FATAL|PANIC|DEBUG|INFO|NOTICE):\s+'
    r'(?P<message>.*)'
)

# Duration pattern for slow query detection
DURATION_PATTERN = re.compile(r'duration:\s+([\d.]+)\s+ms')

# Statement pattern
STATEMENT_PATTERN = re.compile(r'statement:\s+(.+)', re.DOTALL)


def parse_log_line(line: str) -> Optional[dict[str, Any]]:
    """
    Parse a single PostgreSQL log line.

    Handles multiple log_line_prefix formats.

    Args:
        line: Raw log line string

    Returns:
        Dict with parsed fields or None if not a valid log line
    """
    line = line.strip()
    if not line:
        return None

    # Try primary pattern
    match = LOG_LINE_PATTERN.match(line)
    if not match:
        # Try simpler pattern
        match = LOG_LINE_SIMPLE.match(line)

    if not match:
        return None

    result = {
        'timestamp': match.group('timestamp'),
        'pid': int(match.group('pid')),
        'level': match.group('level'),
        'message': match.group('message'),
    }

    # Add optional fields if present
    if 'user' in match.groupdict() and match.group('user'):
        result['user'] = match.group('user')
    if 'db' in match.groupdict() and match.group('db'):
        result['database'] = match.group('db')

    # Extract duration if present (slow query log)
    duration_match = DURATION_PATTERN.search(result['message'])
    if duration_match:
        result['duration_ms'] = float(duration_match.group(1))

    # Extract SQL statement if present
    statement_match = STATEMENT_PATTERN.search(result['message'])
    if statement_match:
        result['statement'] = statement_match.group(1).strip()
        # Truncate very long statements
        if len(result['statement']) > 2000:
            result['statement'] = result['statement'][:2000] + '...[truncated]'

    return result


def tail_postgres_log(
    log_path: str,
    poll_interval: float = 0.1
) -> Generator[dict[str, Any], None, None]:
    """
    Continuously tail PostgreSQL log file.

    Handles log rotation by detecting inode changes.
    Buffers multi-line log entries before yielding.

    Args:
        log_path: Path to PostgreSQL log file
        poll_interval: Seconds to wait when no new data (default: 0.1s)

    Yields:
        Parsed log entry dicts
    """
    logger.info("starting_log_tail", path=log_path)

    # Wait for file to exist
    while not os.path.exists(log_path):
        logger.warning("log_file_not_found", path=log_path)
        time.sleep(5)

    # Open file and seek to end
    f = open(log_path, 'r', encoding='utf-8', errors='replace')
    f.seek(0, os.SEEK_END)
    last_inode = os.fstat(f.fileno()).st_ino

    # Buffer for multi-line entries
    line_buffer: list[str] = []
    timestamp_pattern = re.compile(r'^\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2}')

    while True:
        try:
            line = f.readline()

            if line:
                # Check if this is a new log entry (starts with timestamp)
                if timestamp_pattern.match(line) and line_buffer:
                    # Yield the previous buffered entry
                    full_line = ''.join(line_buffer)
                    parsed = parse_log_line(full_line)
                    if parsed:
                        yield parsed
                    line_buffer = []

                line_buffer.append(line)

            else:
                # No new data, check for log rotation
                try:
                    current_inode = os.stat(log_path).st_ino
                    if current_inode != last_inode:
                        logger.info("log_rotation_detected", path=log_path)
                        # Yield any remaining buffered content
                        if line_buffer:
                            full_line = ''.join(line_buffer)
                            parsed = parse_log_line(full_line)
                            if parsed:
                                yield parsed
                            line_buffer = []

                        # Reopen file
                        f.close()
                        f = open(log_path, 'r', encoding='utf-8', errors='replace')
                        last_inode = current_inode

                except FileNotFoundError:
                    # File deleted during rotation, wait for new file
                    logger.warning("log_file_deleted", path=log_path)
                    f.close()
                    time.sleep(1)
                    while not os.path.exists(log_path):
                        time.sleep(1)
                    f = open(log_path, 'r', encoding='utf-8', errors='replace')
                    last_inode = os.fstat(f.fileno()).st_ino

                time.sleep(poll_interval)

        except Exception as e:
            logger.error("log_tail_error", error=str(e))
            time.sleep(1)


class LogCollector:
    """
    Buffered log collector for batch processing.

    Collects log entries until flushed, returning all accumulated entries.
    """

    def __init__(self, max_entries: int = 1000):
        """
        Initialize collector.

        Args:
            max_entries: Maximum entries to buffer before auto-flush
        """
        self.max_entries = max_entries
        self._entries: list[dict[str, Any]] = []

    def add(self, entry: dict[str, Any]) -> bool:
        """
        Add log entry to buffer.

        Args:
            entry: Parsed log entry

        Returns:
            True if buffer is full (should flush)
        """
        self._entries.append(entry)
        return len(self._entries) >= self.max_entries

    def flush(self) -> list[dict[str, Any]]:
        """
        Get and clear all buffered entries.

        Returns:
            List of buffered entries
        """
        entries = self._entries
        self._entries = []
        return entries

    def count(self) -> int:
        """Get current buffer size."""
        return len(self._entries)
```
  </action>
  <verify>
    - File exists: postgres-agent/src/collectors/log_parser.py
    - Contains parse_log_line function with pattern matching
    - Contains tail_postgres_log generator with rotation handling
    - Handles multi-line log entries (buffers until next timestamp)
    - Extracts duration_ms for slow query detection
  </verify>
  <done>Postgres log parser with rotation handling and multi-line support</done>
</task>

<task type="auto">
  <name>Task 2: Create HTTP client and circuit breaker for transmission</name>
  <files>
    postgres-agent/src/transmission/__init__.py
    postgres-agent/src/transmission/http_client.py
    postgres-agent/src/transmission/circuit_breaker.py
  </files>
  <action>
Create HTTP transmission layer with circuit breaker pattern.

1. Create `postgres-agent/src/transmission/__init__.py`:
   ```python
   """Transmission layer for sending data to listener."""
   from .http_client import send_to_listener
   from .circuit_breaker import get_circuit_breaker, is_circuit_open
   from .buffer import get_buffer, buffer_data, flush_buffer

   __all__ = [
       'send_to_listener',
       'get_circuit_breaker',
       'is_circuit_open',
       'get_buffer',
       'buffer_data',
       'flush_buffer',
   ]
   ```

2. Create `postgres-agent/src/transmission/circuit_breaker.py`:
   ```python
   """
   Circuit breaker for HTTP transmission.

   Opens after 5 consecutive failures, closes after 60 seconds.
   Prevents overwhelming listener during outages.
   """
   from typing import Optional
   from pybreaker import CircuitBreaker, CircuitBreakerError
   import structlog

   logger = structlog.get_logger()

   # Module-level circuit breaker instance
   _breaker: Optional[CircuitBreaker] = None


   def _on_state_change(breaker: CircuitBreaker, old_state: str, new_state: str):
       """Log circuit breaker state changes."""
       logger.warning(
           "circuit_breaker_state_change",
           breaker=breaker.name,
           old_state=old_state,
           new_state=new_state
       )


   def get_circuit_breaker(
       fail_max: int = 5,
       timeout_duration: int = 60
   ) -> CircuitBreaker:
       """
       Get or create the circuit breaker instance.

       Args:
           fail_max: Number of failures before opening (default: 5)
           timeout_duration: Seconds before attempting to close (default: 60)

       Returns:
           CircuitBreaker instance
       """
       global _breaker

       if _breaker is None:
           _breaker = CircuitBreaker(
               fail_max=fail_max,
               reset_timeout=timeout_duration,
               name="listener_http"
           )
           # Add state change listener
           _breaker.add_listener(_on_state_change)
           logger.info(
               "circuit_breaker_created",
               fail_max=fail_max,
               timeout_duration=timeout_duration
           )

       return _breaker


   def is_circuit_open() -> bool:
       """
       Check if circuit breaker is open.

       Returns:
           True if circuit is open (should not attempt requests)
       """
       if _breaker is None:
           return False
       return _breaker.current_state == 'open'


   def reset_circuit_breaker():
       """Reset circuit breaker to closed state (for testing)."""
       global _breaker
       if _breaker is not None:
           _breaker._state = 'closed'
           _breaker._failure_count = 0
   ```

3. Create `postgres-agent/src/transmission/http_client.py`:
   ```python
   """
   HTTP client for sending data to listener server.

   PG-COMM-01: Send collected data to listener server via HTTP POST
   PG-COMM-03: Include project identifier with all sent data
   """
   import json
   import time
   from typing import Any
   import requests
   from pybreaker import CircuitBreakerError
   import structlog

   from ..config import Config
   from .circuit_breaker import get_circuit_breaker
   from .buffer import buffer_data

   logger = structlog.get_logger()


   def send_to_listener(
       data: dict[str, Any],
       config: Config,
       source: str
   ) -> bool:
       """
       Send data to listener with circuit breaker protection.

       Falls back to buffering if circuit is open or request fails.

       Args:
           data: Data payload to send
           config: Agent configuration
           source: Source type (pg_stat_activity, pg_stat_statements, pg_log, system_metrics)

       Returns:
           True if sent successfully, False if buffered
       """
       breaker = get_circuit_breaker(
           fail_max=config.circuit_breaker_fail_max,
           timeout_duration=config.circuit_breaker_timeout_s
       )

       # Build payload matching listener's PostgresPayloadSchema
       payload = {
           'correlation_id': data.get('correlation_id', ''),
           'project': config.project_id,  # PG-COMM-03
           'timestamp': time.time(),
           'source': source,
           'data': data
       }

       try:
           # Attempt send with circuit breaker
           @breaker
           def _send():
               response = requests.post(
                   config.listener_url,
                   json=payload,
                   headers={
                       'Authorization': f'Bearer {config.listener_api_key}',
                       'Content-Type': 'application/json'
                   },
                   timeout=config.listener_timeout_s
               )
               response.raise_for_status()
               return response

           response = _send()

           logger.debug(
               "data_sent_to_listener",
               source=source,
               status=response.status_code,
               bytes=len(json.dumps(payload))
           )
           return True

       except CircuitBreakerError:
           logger.warning(
               "circuit_open_buffering",
               source=source
           )
           buffer_data(payload)
           return False

       except requests.RequestException as e:
           logger.warning(
               "listener_request_failed_buffering",
               source=source,
               error=str(e)
           )
           buffer_data(payload)
           return False

       except Exception as e:
           logger.error(
               "send_to_listener_unexpected_error",
               source=source,
               error=str(e)
           )
           buffer_data(payload)
           return False


   def send_batch_to_listener(
       items: list[dict[str, Any]],
       config: Config,
       source: str
   ) -> tuple[int, int]:
       """
       Send batch of items to listener.

       Args:
           items: List of data items
           config: Agent configuration
           source: Source type

       Returns:
           Tuple of (sent_count, buffered_count)
       """
       sent = 0
       buffered = 0

       for item in items:
           if send_to_listener(item, config, source):
               sent += 1
           else:
               buffered += 1

       return sent, buffered
   ```
  </action>
  <verify>
    - File exists: postgres-agent/src/transmission/__init__.py
    - File exists: postgres-agent/src/transmission/circuit_breaker.py
    - File exists: postgres-agent/src/transmission/http_client.py
    - circuit_breaker.py creates CircuitBreaker with fail_max=5, reset_timeout=60
    - http_client.py sends Bearer token in Authorization header
    - http_client.py falls back to buffer_data on failure
    - Payload matches listener's PostgresPayloadSchema
  </verify>
  <done>HTTP client with circuit breaker pattern and fallback to buffering</done>
</task>

<task type="auto">
  <name>Task 3: Create persistent buffer with size limits</name>
  <files>
    postgres-agent/src/transmission/buffer.py
  </files>
  <action>
Create persistent buffer using persist-queue with size limits and eviction.

Create `postgres-agent/src/transmission/buffer.py`:
```python
"""
Persistent buffer for offline resilience.

Uses SQLite-backed queue for crash recovery.
Implements size limits with oldest-item eviction.

PG-COMM-02: Implement local buffering for listener unavailability
"""
import json
import os
from typing import Any, Optional
from persistqueue import FIFOSQLiteQueue
import structlog

from ..config import Config

logger = structlog.get_logger()

# Module-level buffer instance
_buffer: Optional[FIFOSQLiteQueue] = None
_config: Optional[Config] = None


def init_buffer(config: Config) -> FIFOSQLiteQueue:
    """
    Initialize the persistent buffer.

    Creates buffer directory if it doesn't exist.

    Args:
        config: Agent configuration

    Returns:
        FIFOSQLiteQueue instance
    """
    global _buffer, _config
    _config = config

    # Ensure buffer directory exists
    os.makedirs(config.buffer_path, exist_ok=True)

    _buffer = FIFOSQLiteQueue(
        path=config.buffer_path,
        multithreading=True,
        auto_commit=True
    )

    logger.info(
        "buffer_initialized",
        path=config.buffer_path,
        max_size_mb=config.buffer_max_size_mb
    )

    return _buffer


def get_buffer() -> FIFOSQLiteQueue:
    """
    Get the current buffer instance.

    Raises:
        RuntimeError: If buffer not initialized

    Returns:
        FIFOSQLiteQueue instance
    """
    if _buffer is None:
        raise RuntimeError("Buffer not initialized. Call init_buffer() first.")
    return _buffer


def _check_and_evict_if_needed():
    """
    Check buffer size and evict oldest items if over limit.

    Evicts until buffer is at 80% of max size.
    """
    if _buffer is None or _config is None:
        return

    db_path = os.path.join(_config.buffer_path, "data")
    if not os.path.exists(db_path):
        # Try alternative path
        for fname in os.listdir(_config.buffer_path):
            if fname.endswith('.sqlite') or fname.endswith('.db'):
                db_path = os.path.join(_config.buffer_path, fname)
                break
        else:
            return

    try:
        size_mb = os.path.getsize(db_path) / (1024 * 1024)

        if size_mb > _config.buffer_max_size_mb:
            logger.warning(
                "buffer_size_exceeded_evicting",
                current_mb=round(size_mb, 2),
                max_mb=_config.buffer_max_size_mb
            )

            # Evict until at 80% of max
            target_mb = _config.buffer_max_size_mb * 0.8
            evicted = 0

            while size_mb > target_mb and _buffer.qsize() > 0:
                try:
                    _buffer.get(block=False)
                    _buffer.task_done()
                    evicted += 1
                    size_mb = os.path.getsize(db_path) / (1024 * 1024)
                except Exception:
                    break

            logger.info(
                "buffer_eviction_complete",
                evicted_count=evicted,
                new_size_mb=round(size_mb, 2)
            )

    except Exception as e:
        logger.error("buffer_size_check_failed", error=str(e))


def buffer_data(data: dict[str, Any]) -> bool:
    """
    Add data to buffer with size limit enforcement.

    Args:
        data: Data payload to buffer

    Returns:
        True if buffered successfully
    """
    if _buffer is None:
        logger.error("buffer_not_initialized")
        return False

    try:
        # Check and evict if needed before adding
        _check_and_evict_if_needed()

        # Serialize and buffer
        _buffer.put(json.dumps(data))

        logger.debug(
            "data_buffered",
            queue_size=_buffer.qsize()
        )
        return True

    except Exception as e:
        logger.error("buffer_put_failed", error=str(e))
        return False


def flush_buffer(config: Config, max_items: int = 100) -> tuple[int, int]:
    """
    Attempt to flush buffered data to listener.

    Called when circuit breaker closes or periodically.

    Args:
        config: Agent configuration
        max_items: Maximum items to flush in one call

    Returns:
        Tuple of (sent_count, remaining_count)
    """
    if _buffer is None:
        return 0, 0

    # Import here to avoid circular dependency
    from .http_client import send_to_listener
    from .circuit_breaker import is_circuit_open

    sent = 0
    remaining = _buffer.qsize()

    if remaining == 0:
        return 0, 0

    logger.info("flushing_buffer", items=min(remaining, max_items))

    for _ in range(min(remaining, max_items)):
        if is_circuit_open():
            logger.warning("flush_stopped_circuit_open")
            break

        try:
            item_str = _buffer.get(block=False)
            item = json.loads(item_str)

            # Re-send using http_client (will re-buffer on failure)
            source = item.get('source', 'buffered')
            data = item.get('data', item)

            # Direct request without circuit breaker (already checked above)
            import requests
            response = requests.post(
                config.listener_url,
                json=item,
                headers={
                    'Authorization': f'Bearer {config.listener_api_key}',
                    'Content-Type': 'application/json'
                },
                timeout=config.listener_timeout_s
            )

            if response.status_code == 200:
                _buffer.task_done()
                sent += 1
            else:
                # Put back in queue
                _buffer.put(item_str)
                logger.warning(
                    "flush_item_failed",
                    status=response.status_code
                )
                break

        except Exception as e:
            logger.error("flush_item_error", error=str(e))
            break

    remaining = _buffer.qsize()
    logger.info(
        "buffer_flush_complete",
        sent=sent,
        remaining=remaining
    )

    return sent, remaining


def get_buffer_stats() -> dict[str, Any]:
    """
    Get buffer statistics.

    Returns:
        Dict with queue_size and size_mb
    """
    if _buffer is None:
        return {"status": "not_initialized"}

    stats = {
        "queue_size": _buffer.qsize(),
        "status": "initialized"
    }

    # Try to get file size
    if _config:
        try:
            for fname in os.listdir(_config.buffer_path):
                if fname.endswith('.sqlite') or fname.endswith('.db') or fname == 'data':
                    db_path = os.path.join(_config.buffer_path, fname)
                    if os.path.exists(db_path):
                        stats["size_mb"] = round(
                            os.path.getsize(db_path) / (1024 * 1024), 2
                        )
                        break
        except Exception:
            pass

    return stats


def close_buffer():
    """Close the buffer (call during shutdown)."""
    global _buffer
    if _buffer is not None:
        logger.info("closing_buffer")
        _buffer = None
```
  </action>
  <verify>
    - File exists: postgres-agent/src/transmission/buffer.py
    - Uses FIFOSQLiteQueue from persistqueue
    - Implements size limit checking (_check_and_evict_if_needed)
    - Evicts oldest items when buffer exceeds max_size_mb
    - flush_buffer respects circuit breaker state
    - get_buffer_stats returns queue size and file size
  </verify>
  <done>Persistent buffer with SQLite backing, size limits, and oldest-item eviction</done>
</task>

</tasks>

<verification>
1. Log parser handles rotation and multi-line entries
2. HTTP client uses Bearer token authentication
3. Circuit breaker opens after 5 failures, resets after 60 seconds
4. Buffer uses persist-queue SQLite backend
5. Buffer evicts oldest items when exceeding size limit
6. flush_buffer respects circuit breaker state
</verification>

<success_criteria>
- Postgres log parser continuously parses with rotation handling (PG-03)
- HTTP client sends with Bearer auth to /ingest/postgres (PG-COMM-01)
- Circuit breaker opens after 5 failures
- Buffer stores data during outages (PG-COMM-02)
- Buffer enforces size limit with eviction
</success_criteria>

<output>
After completion, create `.planning/phases/05-postgres-agent-database-monitoring/05-03-SUMMARY.md`
</output>
